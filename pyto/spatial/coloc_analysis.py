"""
Generation, preprocessing and analysis of colocalization results

Attributes that hold colocalization data are of the form:
  - 3-colocalization, all synapses together: pre_tether_post_data
  - 3-colocalization, individual synapses: pre_tether_post_data_syn

Other attributes:

Methods to read and preprocess data:

Superseed colocalization.py module.
 
# Author: Vladan Lucic (Max Planck Institute for Biochemistry)
# $Id$
"""

__version__ = "$Revision$"

import os
import sys
import subprocess
import pickle
import itertools
import time
import re
import functools

import numpy as np
import scipy as sp
import pandas as pd 
import matplotlib as mpl
import matplotlib.pyplot as plt

import pyto
from . import coloc_functions as col_func


class ColocAnalysis(object):
    """
    Provides class methods to generate raw colocalization data by running
    an external PySeg colocalization script and preprocess raw colocalization
    data. The resulting colocalization data are saved in an object of this 
    class.

    An instance of this class contains colocalization data for one
    colocalization project.

    A colocalization project comprises multiple colocalization, where each 
    colocalization is defined by a distance and a colocalization case.
   
    Colocalization cases are obtained by colocalizing different particle 
    sets. The data for each coloclalization case comprizes two (pandas) 
    data tables, saved as the following attributes:
      - name_data: combined data for all tomograms, one row for each distance
      - name_data_syn: data for individual tomograms, one row for each
      combination of tomogram and colocalization distance
    where name is a colocalization name.

    Colocalization names are formed by concatenating particle set names with
    '_' placed between the names, such as (pre_post_tether or setX_setY).
    Particle set names should not contain  '_'.

    Note that colocalize_run() requires coloclization names with the number
    of particle sets prepended (such as 3_pre_post_tether) or 2_setX_setY).
    
    Table columns (of both individual and combined tomos tables) obtained 
    directly from the coloclization script run:
      - distance: colocalization distance [nm]
      - n_subcol: number of colocalizations (subcolumns)
      - n_set{0, 1, 2}_subcol: number of particles of a set in subcolumns
      - n_set{0, 1, 2}_total: total number of particles of a set  
      - n_col: number of columns (formed by subcolumns)
      - area, volume: total area [nm^2], volume [pixel]  of the voi where 
      particles are projected
      - area_col: area of the voi (as above) occupied by columns
      - n_subcol_random_all: number of subcolumns by random simulations
      where particles of set0 is kept fixed and particles of sets 1 and 2 
      are randomly distributed (inverse?)
      - n_subcol_random_alt_all: number of subcolumns by random simulations
      where particles of sets 1 and 2 are kept fixed and particles of set0 
      are randomly distributed (inverse?)
      - n_set{0, 1, 2}_subcol_random (only individual tomo table)
      - n_teth_cent, n_sv: should not be used
      
    Additional columns generated during preprocessing:
      - n_subcol_random_{mean, std}: mean, std of n_subcol_random_all
      - n_subcol_random_alt_{mean, std}: mean, std of n_subcol_random_alt_all
      - n_subcol_random_combined_{mean, std}: : mean, std for 
      n_subcol_random_all and n_subcol_random_alt_all taken together
      - p_subcol_normal: p-value for random simulations (fraction of 
      n_subcol_random_all smaller than n_subcol)
      - p_subcol_other: p-value for random_alt simulations (fraction of 
      n_subcol_random_alt_all smaller than n_subcol)
      - p_subcol_combined: p-value for random and random_alt simulations 
      taken together
      - density (only combined tomos):

    Columns generated by combine_permuted():
      - p_subcol_{normal, other, combined}_3: p-values for normal, other
      and combined cases like above, but here colocalizations obtained 
      from the same particle sets but in all available permuted order
      are used
    """

    def __init__(
            self, dir_=None, pick=None, dir_prefix='tables_',
            join_suffix='data', individual_suffix='data_syn', mode=None):
        """
        Sets parameters.

        Arguments:
          - dir_: tables directory
          - pick: name of this colocalization project (used if dir_ is None)
          - dir_prefix: tables directory prefix
          - join_suffix: suffix given to colocalization data tables that 
          contain data for all tomos together
          - individual_suffix: suffix given to colocalization data tables that 
          contain data for individual tomos
         - mode: defines how to form the paths to the raw colocalization 
          data and defines how to extract tomo id (see extract_data() doc)
        """

        # suffices given to colocalization data tables that contain data for
        # all tomos together and for individual tomos
        self.join_suffix = join_suffix
        self.individual_suffix = individual_suffix

        # function used to determine p-values
        self.p_func = np.greater

        # file organization mode
        self.mode = mode
        
        # resolve tables directory, if possible
        if dir_ is None:
            if pick is not None:
                self.tables_dir = dir_prefix + pick
        else:
            self.tables_dir = dir_

    def copy_setup(self):
        """
        Returns an instance of this class that has the same attributes
        as this instance, except that it does not have any data attribute
        nor self._names.
        """
        
        new = self.__class__()
        new.join_suffix = self.join_suffix
        new.individual_suffix = self.individual_suffix
        new.p_func = self.p_func
        new.mode = self.mode
        try:
            new.tables_dir = self.tables_dir
        except AttributeError: pass
            
        return new
            
    ###########################################################
    #
    # Class method that runs colocalization
    #

    @classmethod
    def colocalize_run(
            cls, coloc_case, psets, script_name, distances, n_sim, root_path,
            mode, column_radius_factor, pixel_size=1.,
            python_cmd='python3', debug=False):
        """
        Runs colocalization script to generate raw colocalization data.

        Prepares variables that are passed to the colocalization script (the
        script is modified from the PySeg version so that it accepts 
        arguments). The script is run in parallel, for each distance 
        separately.

        Argumens passed to the script:
          - argv[0]: script name
          - argv[1], argv[3], argv[5]: particle set files 
          - argv[2], argv[4], argv[6]: particle set ids 
          - argv[7]: particle shape
          - argv[8]: path to the raw colocalization data directory 
          - argv[9]: raw colocalization data stem
          - argv[10]: colocalization distance 
          - argv[11]: columns distance 
          - argv[12]: n simulations
          - argv[13]: pixel size [nm]

        Arguments:
          - coloc_case: colocalization name with the prepended number of 
          colocalizations (e.g. '3_setX_setY_setZ', '2_pre_post')
          - pset: module containing paths to particle set star files and ids_1
          - script_name: path to PySeg colocalization script 
          - distances: list of colocalization distances (in nm)
          - n_sim: number of simulations
          - root path: raw colocalization results root path
          - mode: defines how to form the paths to the raw colocalization data
          (see extract_data() doc)
          - column_radius_factor: column radius is obtained by multiplying 
          the colocalization distance by this factor
          - pixel_size: pixel size [nm]
          - python_cmd: python command
          - debug: debug flag, currently not used
        """

        # parse and check coloc_case
        out_base_split = coloc_case.split('_')
        n_coloc = int(out_base_split[0])
        if len(out_base_split)-1 != n_coloc:
            raise ValueError(
                "Variable out_base has {} parts but starts with {}".format(
                    len(out_base_split)-1, n_coloc))
        name = coloc_case.split('_', 1)[1]

        # assign partice sets variables
        in_star_1 = getattr(psets, 'in_star_' + out_base_split[1])
        l_id1 = getattr(psets, 'l_id_' + out_base_split[1])
        in_star_2 = getattr(psets, 'in_star_' + out_base_split[2])
        l_id2 = getattr(psets, 'l_id_' + out_base_split[2])
        if n_coloc == 3:
           in_star_3 = getattr(psets, 'in_star_' + out_base_split[3])
           l_id3 = getattr(psets, 'l_id_' + out_base_split[3])
        elif n_coloc ==2:
           in_star_3 = getattr(psets, 'in_star_' + out_base_split[2])
           l_id3 = getattr(psets, 'l_id_' + out_base_split[2])
        else:
            raise ValueError("Variable out_base has to start with 2 or 3")

        # common arguments
        args_begin = [python_cmd, script_name]
        args_begin += [
            in_star_1, str(l_id1), in_star_2, str(l_id2), in_star_3, str(l_id3)]
        args_begin += [psets.in_part]

        #
        base_paths = col_func.get_raw_coloc_bases(
            name=name, distances=distances, mode=mode, n_sim=n_sim,
            n_coloc=n_coloc, column_radius_factor=column_radius_factor)
        
        for dist, b_path in zip(distances, base_paths):
            column_radius = column_radius_factor * dist

            # add arguments that depend on distance
            path = os.path.normpath(os.path.join(root_path, b_path))
            out_dir, out_stem = os.path.split(path)
            args = args_begin + [
                out_dir, out_stem, str(dist), str(column_radius),
                str(n_sim), str(pixel_size)]

            # make log path
            log_name = \
                f'{n_coloc}_{name}_{dist}_{column_radius}_sim-{n_sim}.log'
            #out_dir = os.path.join(
            #    out_root_path, out_base, f'dist-{dist}_cr-{column_radius}')
            #out_dir = os.path.normpath(out_dir)
            #out_stem =  '{}_sim-{}'.format(out_base, n_simulations)
            #log_file_name = os.path.join(out_dir, out_stem + '.log')
            log_fd = open(log_name, 'w')

            # run
            if debug:
                print(args)
            subprocess.Popen(args, stdout=log_fd, stderr=log_fd)
            # in some cases necessary for subprocesses to run (don't know why)
            time.sleep(5)
 
    
    ###########################################################
    #
    # Class methods that read and preprocess raw colocalization data and
    # return an instance of this class
    #

    @classmethod
    def extract_multi(
            cls, names, distances, in_path, mode, n_sim=None, columns=None,
            p_values=True, random_stats=False, save=False,
            force=False,  dir_='.', verbose=True,
            join_suffix='data', individual_suffix='data_syn'):
        """
        Reads multiple raw colocalization results (arg names) for all distances 
        (arg distances) from raw data (workspaces), preprocess this raw data
        and converts it to tables.

        For each colocalization case, the preprocessed data is saved in
        two tables (pd.DataFrame). One table contains the data for all
        tomos together and the other for individual tomos. An instance
        of this class is generated and the tables are saved as attributes 
        of this instance. Furthermore, the tables are pickled and saved. 

        The colocalization results are assigned to variables (pandas.DataFrame),
        two for each colocalization case. One variable contains data for all
        tomograms together and the other for individual tomograms. Each 
        variable contains results for all distances.

        The variable names are obtained from colocalization cases (elements
        of arg names) as follows:
          - colocalization_case + '_data': all tomograms together
          - colocalization_case + '_data_syn': individual tomograms
          - _names: all colocalization names (from arg names)
        They are sorted by tomo name ('_data_syn') and by distance (both).

        If arg columns is not None, the variable containing data from all 
        tomograms together will show only the specified columns. 

        If arg save is True, the variables may be pickled and saved in
        directory specified by arg dir_. If arg force is True, all variables
        are pickled and saved (overwrites files), if force is False, only 
        those variables that were not saved before are saved.

        The pickle file names are obtained by appending '.pkl' to varable names.

        Arguments:
          - names: list of colocalization names
          - distances: list of distances
          - in_path: path to the root of colocalization results
          - mode: defines how to form the paths to the raw colocalization 
          data and defines how to extract tomo id (see extract_data() doc)
          - module: module for the created variables
          - columns: list of columns
          - p_values: flag indication if p_values should be calculated
          - random_stats: Flag indicating if basic statistics on random data
          is calculated
          - save: Flag indicating wether the colocalization data 
          (variables) is pickled
          - force: if False, the existing colocalization data pickles
          are now overwriten
          - dir_: directory where the pickled preprocessed tables are written
          - verbose: Prints info about safed files and data that is not found
          - join_suffix: suffix given to colocalization data tables that 
          contain data for all tomos together
          - individual_suffix: suffix given to colocalization data tables that 
          contain data for individual tomos
        """

        obj = cls(
            join_suffix=join_suffix, individual_suffix=individual_suffix,
            mode=mode)
        names_local = list(set(names))
        names_local.sort()
        obj._names = names_local
        for nam in names_local:

            # set variable names
            obj_name = nam + '_' + obj.join_suffix
            obj_name_syn = nam + '_' + obj.individual_suffix

            # check if saved already
            if save:
                out_path = os.path.join(dir_, obj_name+'.pkl')
                out_path_syn = os.path.join(dir_, obj_name_syn+'.pkl')
                exists = (
                    (os.path.exists(out_path) and os.path.exists(out_path_syn)))

            # read data
            try:
                data, data_syn = obj.extract_data(
                    name=nam, distances=distances, mode=mode, in_path=in_path,
                    n_sim=n_sim, random_stats=random_stats, p_values=True)  
            except FileNotFoundError as err:
                if verbose:
                    print('Colocalization {} not found'.format(nam))
                    #print(format(err))
                continue
            col_func.calculate_density(data)

            # make variable and save data for all tomos together, if needed
            if save and (force or not exists):
                try:
                    pd.to_pickle(data, out_path)
                except (FileNotFoundError, OSError):
                    if not os.path.exists(dir_):
                        os.makedirs(dir_)
                        pd.to_pickle(data, out_path)
                    else:
                        raise
                if verbose:
                    print("Saved colocalization {}".format(nam))
            if columns is not None:
                columns_local = [col for col in columns if col in data.columns]
                data = data[columns_local]
            setattr(obj, obj_name, data)

            # make variable and save data for individual tomos, if needed
            if save and (force or not exists):
                pd.to_pickle(data_syn, out_path_syn)
            setattr(obj, obj_name_syn, data_syn)

        return obj

    @classmethod
    def read_table_multi(
            cls, dir_=None, pick=None, names=None, columns=None,
            verbose=True, suffix='_data.pkl',
            join_suffix='data', individual_suffix='data_syn', 
            mode=None):
        """
        Reads preprocessed colocalization results from pickled tables.

        Each table is assigned to a variable having the same name as the 
        table file base name without extension. In this way it is the variable
        and table file names are the same as in extract_multi().

        Two variables are set for each name (element of arg names):
          - name + "_data" : data for all tomograms together
          - name + "_data_syn": data for individual tomograms
        Variables are set as attributes of object specified by arg module,
        where arg module can also be a module. 

        If arg columns is not None, the variable containing data from all 
        tomograms together will show only the specified columns. 

        Arguments:
          - names: list of colocalization names corresponding to the data 
          to be read, if None all data in tables directory are read  
          - module: module for the created variables
          - dir_: directory where tables are saved
          - pick: name of this colocalization project (used if dir_ is None)
          - columns: list of columns
          - suffix: pickled tables name suffix 
          - verbose: If True, prints names of tables that are not found 
          - join_suffix: suffix given to colocalization data tables that 
          contain data for all tomos together
          - individual_suffix: suffix given to colocalization data tables that 
          contain data for individual tomos
          - mode: defines how to form the paths to the raw colocalization 
          data and defines how to extract tomo id (see extract_data() doc)
         """

        if (dir_ is None) and (pick is None):
            raise ValueError("Either pick or dir_ argument has to be given")
        #thismodule = sys.modules[__name__]
        obj = cls(
            dir_=dir_, pick=pick, mode=mode,
            join_suffix=join_suffix, individual_suffix=individual_suffix)

        # get all names
        if names is None:
            names = [
                file_.split(suffix)[0] for file_ in os.listdir(obj.tables_dir)
                if file_.endswith(suffix)]
        obj._names = names
        
        for nam in names:

            try: 

                # joint data
                obj_name = nam + '_' + obj.join_suffix
                path = os.path.join(obj.tables_dir, obj_name+'.pkl')
                data = pd.read_pickle(path)
                if columns is not None:
                    columns_local = [
                        col for col in columns if col in data.columns]
                    data = data[columns_local]
                setattr(obj, obj_name, data)

                # individual tomo data
                obj_name_syn = nam + '_' + obj.individual_suffix
                path = os.path.join(obj.tables_dir, obj_name_syn+'.pkl')
                data_syn = pd.read_pickle(path)
                setattr(obj, obj_name_syn, data_syn)

            except FileNotFoundError:
                if verbose:
                    print('Colocalization {} not found'.format(nam))

        return obj
    
    @classmethod
    def read_directly(
            cls, name, distance, in_path, mode, index, ids=None, n_sim=None):
        """
        Reads the specified workspace record directly.

        Used for testing function extract_data() and related.

        Arguments:
          - name: name of the colocalization
          - distance: single distance
          - in_path: root path of the workspaces (pickles)
          - mode: defines how to form the paths to the raw colocalization 
          data and defines how to extract tomo id (see extract_data() doc)
           - index: index of the workspace element that is read
          - ids: tomo ids for which the data is read, if None data for all 
          tomos is read

        Return: dictionary wher keys are tomo ids and values are the data 
        records
        """

        #obj = cls(mode=mode)
        (wspaces, _, _, _) = col_func.set_read_parameters(
            name=name, distances=[distance], in_path=in_path, mode=mode,
            n_sim=n_sim)

        #pkl_path = os.path.join(in_path, wspaces[0])
        pkl_path = wspaces[0]
        pkl_data = pickle.load(open(pkl_path, 'rb'), encoding='latin1')

        data = {}
        for tomo_name, value in pkl_data[index].items():
            syn_id = col_func.get_tomo_id(tomo_name, mode=mode)
            data[syn_id] = value

        result = {}    
        if ids is None:
            result = data
        else:
            for id_ in ids:
                try:
                    value = data[id_]
                except KeyError:
                    value = np.nan
                result[id_] = value

        return result

    ###########################################################
    #
    # Basic data manipulation
    #

    def get_data(self, name):
        """
        Returns all tomos and individual tomos data tables for given 
        colocalization name (arg name).

        Argument:
          - name: colocalization name

        Returns: 
          (data_table_all_tomos together, data_table_individual_tomos)
        """

        join_data = getattr(self, self.get_join_name(name))
        indiv_data = getattr(self, self.get_individual_name(name))
        return join_data, indiv_data

    def get_join_name(self, name):
        """
        Returns name of the attribute holding data for all tomos together
        """
        return name + '_' + self.join_suffix
    
    def get_individual_name(self, name):
        """
        Returns name of the attribute holding data for individual tomos
        """
        return name + '_' + self.individual_suffix

    def add_data(self, name, data, data_syn):
        """
        Adds specified data 
        """
        join_name = self.get_join_name(name=name)
        individ_name = self.get_individual_name(name=name)
        setattr(self, join_name, data)
        setattr(self, individ_name, data_syn)
        try:
            self._names.append(name)
        except (NameError, AttributeError):
            self._names = [name]
            
        
    ###########################################################
    #
    # Methods needed to read and preprocess data
    #

    def add_random_stats(self, names=None, n_particles=False):
        """
        Calculates basic statistics (mean and std) for random simulations
        and adds it to the data tables, for multiple 3-colocalizations
        at all distances.

        Note: Provides similar functionality to what is in extract_data(),
        but differs in that this method takes multiple colocalization names
        as argument, while extract_data() works only on one colocalization
        case.

        Argument:
          - names: list of colocalization names
          - n_particles: flag indicating wheteher stats for number of 
          particles in subcolumns is calculated

        Random data columns:
          - n_subcol_random_all
          - n_subcol_random_alt_all
          - particles in subcolumns for all three layers separately (if
          n_particles is True)

        The data is added to both individual synapses and synapses 
        taken together data tables.
        """

         # get names
        if names is None: names = self._names
        
        # loop over all colocalizations
        for nam in names:

            # get data and initialize for this colocalization
            data_name = self.get_join_name(nam)
            data_syn_name = self.get_individual_name(nam)
            data = getattr(self, data_name)
            data_syn = getattr(self, data_syn_name)

            # n subcolumn
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, column='n_subcol_random_all', 
                out_column='n_subcol_random', combine=False)
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, column='n_subcol_random_alt_all', 
                out_column='n_subcol_random_alt', combine=False)
            data, data_syn = col_func.get_random_stats(
                data=data, data_syn=data_syn, 
                column=['n_subcol_random_all','n_subcol_random_alt_all'], 
                out_column='n_subcol_random_combined', combine=True)

            # particles in subcolumn
            if n_particles:
                pset_names = col_func.get_layers(name=nam)
                for layer_name in set(pset_names):
                    random_col_name = 'n_' + layer_name + '_subcol_random'
                    data, data_syn = col_func.get_random_stats(
                        data=data, data_syn=data_syn, column=random_col_name, 
                        out_column=random_col_name, combine=False)

            # save data
            setattr(self, data_name, data)
            setattr(self, data_syn_name, data_syn)

    def extract_data(
            self, name, distances, in_path, mode='method-1', n_sim=None,
            p_values=True, random_stats=False):
        """
        Extracts raw data for multiple distance (arg distances) of one 
        colocalization raw data specified by (arg name) from workspaces. 

        The data is extracted for each tomo separately and for all 
        tomos together.

        Each raw data workspace contains data for one colocalization case 
        and one colocalization distance, and for all tomos. It is a 
        pickled object that is indexed by data index (see 
        coloc_functions.set_read_parameters() code) and by a string that
        contains tomo name (such as tomo path).

        The raw data workspaces have to be located below (arg) in_path and  
        organized depending on (arg) mode, as follows (see :

        mode='method-1':
            3_name/aln_distance/3_name_sim200
            (e.g. 3_set0_set1_set2/aln_15/3_set0_set1_set2_sim200)

        mode='method-1_cr-same':
            3_name/aln_distance_cr_distance/3_name_sim200
            (e.g. 3_set0_set1_set2/aln_15_cr_30/3_set0_set1_set2_sim200)

        mode='method-1_cr-double':
            n_name/aln_distance_cr-coldist/n_name_sim200_wspace.pkl
            (e.g. 2_set0_set2/aln_15_cr-30/2_set0_set2_sim200_wspace.pkl,
            3_set0_set1_set2/aln_15_cr-30/3_set0_set1_set2_sim200_wspace.pkl)

        mode='method-1_cr-double_v2':
            n_name/dist-distance_cr-coldist/n_name_sim-n_sim_wspace.pkl
            (e.g. 2_set1_set2/dist-10_cr-20/3_set1_set2_sim-200_wspace.pkl,
            3set0_set1_set2/dist-20_cr-40/3_set0_set1_set2_sim-200_wspace.pkl)

        mode='simple_cr-double' or mode='munc13':
            n_name/dist-distance_cr-coldist/n_name_sim-n_sim_wspace.pkl
            (e.g. 2_set2_set1/dist-15_cr-30/3_set2_set1_sim-200_wspace.pkl,
            3_set0_set1_set2/dist-15_cr-30/3_set0_set1-set2_sim-200_wspace.pkl)

        where:
          - n: arg name, that is the number of colocalization layers (2 or 3)
          - name: specifies the layers that are aligned (e.g. 
          'pre_tether_pst', or 'pre_tether')
          - n_coloc_name examples: '3_pre_tether_pst', '2_pre_tether'
          - distance: distance (e.g. 15), has to be an element of arg distances 
          - coldist: column_radius_factor * dist (e.g. 30).
          - n-sim: arg n_sim, that is number of simulations (e.g. 200)

        Arg mode also determines how is tomo id (saved in column id) 
        extracted from the tomo file paths specified in raw data workspaces 
        (keys of dictionaries that are pickled to make the raw data 
        workspaces), as follows (see coloc_functions.get_tomo_id()):

        mode='simple_cr-double':
          path = 'dir1/dir2/base_name.ext'
          tomo id = base_name

        mode='munc13':
          path = 'dir1/dir2/foo_syn_tomoid_bin-foo.ext'
          tomo id = tomoid

        all other cases:
          path = 'dir1/dir2/foo_tomoid1_tomoid2_foo.ext'
          tomo id = tomoid1_tomoid2

        Furthermore, if (arg) mode is 'munc13', a default index is used 
        (ints), while in all other cases tomo id (column id) is used as 
        index (implemented in coloc_functions.read_data()).

        Arguments:
          - name: name of the colocalization
          - distances: list of distances
          - in_path: root path to the colocalizationn raw data
          - mode: type of colocalization raw data path
          - p_values: flag indication if p_values should be calculated
          - random_stats: Flag indicating if basic statistics on random data
          is calculated

        Returns data, data_syn:
          - data: data for all synapses together, sorted by tomo names 
          (pandas.DataFrame)
          - data_syn: data for individual synapses, sorted by tomo names and 
          distances (pandas.DataFrame)
        """

        # set mode
        if mode is None:
            mode = self.mode
            if mode is None:
                raise ValueError(
                    "Mode has to be specified as argument or argument")

        # constants related to the organization of data in workspaces
        (in_wspaces, columns, add_columns,
         array_columns) = col_func.set_read_parameters(
             name=name, distances=distances, in_path=in_path, mode=mode,
             n_sim=n_sim)

        # extract data from all workspaces for all distances
        for dist, wspace in zip(distances, in_wspaces):

            # read data from pickles, individual synapses
            data_syn_local = col_func.read_data(
                pkl_path=wspace, columns=columns, mode=mode)
            data_syn_local.insert(
                0, column='distance', value=dist, allow_duplicates=False)
            #data_syn_local = data_syn_local.set_index('distance')

            # calculate p values, individual synapses 
            if p_values:
                data_syn_local = col_func.get_fraction_random(
                    data=data_syn_local)

            # calculate values for all synapses together
            data_local = col_func.aggregate(
                data=data_syn_local, distance=dist, add_columns=add_columns, 
                array_columns=array_columns, p_values=p_values,
                random_stats=random_stats)

            # calculate basic stats for random n subcolumns 
            if random_stats:
                data_local, data_syn_local = col_func.get_random_stats(
                    data=data_local, data_syn=data_syn_local,
                    column='n_subcol_random_all', out_column='n_subcol_random',
                    combine=False)
                data_local, data_syn_local = col_func.get_random_stats(
                    data=data_local, data_syn=data_syn_local,
                    column='n_subcol_random_alt_all', 
                    out_column='n_subcol_random_alt',
                    combine=False)
                data_local, data_syn_local = col_func.get_random_stats(
                    data=data_local, data_syn=data_syn_local,
                    column=['n_subcol_random_all', 'n_subcol_random_alt_all'],
                    out_column='n_subcol_random_combined',
                    combine=True)

            # update 
            try:
                #data = data.append(data_local, ignore_index=True)
                data = pd.concat([data, data_local], ignore_index=True)
            except NameError:
                data = data_local
            try:
                data_syn = pd.concat(
                    [data_syn, data_syn_local], axis=0, ignore_index=True)
            except NameError:
                data_syn = data_syn_local

        # sort by tomo name and distance
        data = data.sort_values(by='distance')
        data_syn = data_syn.sort_values(by=['distance', 'id'])
                
        return data, data_syn 

    
    ####################################################
    #
    # Colocalization analysis (data processing) of preprocessed
    # coloclization data
    #

    def full_split_by_groups(
            self, id_group, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """
        Extracts and returns data for all colocalization cases of this 
        instance (both the individual and all tomos tables) that contain 
        only the data for tomos specified by another table (args id_group, 
        group_name group_label and id_label) and distances (arg distance). 

        This method extends (and is based on) select_by_group(). There are 
        two differences:
          - This method extract data for all colocalization cases
          present in this instance
          - This method extracts colocalization data for all group names
          existing in id_labels table and returns separate objects for
          each group.

        For all other details please see select_by_group() and select() 
        methods.

        Arguments:
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns dictionary where keys are group names and values are 
        corresponding instances of this class. If a group of the id_group
        table does not exist in this instance, the value of the returned
        dictionary for that group is set to None.
        """

        # loop over groups
        result = {}
        g_names = id_group[group_label].unique()
        for g_nam in g_names:

            # make object for this group and set data
            curr_obj = self.copy_setup()
            found = False
            for name in self._names:
                data, data_sep = self.select_by_group(
                    name=name, id_group=id_group, group_name=g_nam,
                    group_label=group_label, id_label=id_label,
                    distance=distance,
                    p_values=p_values, random_stats=random_stats)
                curr_obj.add_data(name=name, data=data, data_syn=data_sep)
                if (data is not None) and (data_sep is not None):
                    found = True
            if not found:
                curr_obj = None
            result[g_nam] = curr_obj

        return result
        
    def split_by_groups(
            self, name, id_group, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """
        Extracts and returns data for one colocalization case of this 
        instance (both the individual and all tomos tables) that contain 
        only the data for tomos specified by another table (args id_group, 
        group_name group_label and id_label) and distances (arg distance). 

        This method extends (and is based on) select_by_group() except that
        it extracts colocalization data for all group names
        existing in id_labels table and returns separate objects for
        each group.

        For all other details please see select_by_group() and select() 
        methods.

        Arguments:
          - name: name of the colocalization case
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns (all_tomos_dict, individual_tomos_dict), where the two 
        returned dictionaries have group names as keys and the all tomos
         and individual tomos tables, respectively, as values.
        """

        # loop over groups, select tomos
        data_dict = {}
        data_sep_dict = {}
        g_names = id_group[group_label].unique()
        for g_nam in g_names:
            data, data_sep = self.select_by_group(
                name=name, id_group=id_group, group_name=g_nam,
                group_label=group_label, id_label=id_label, distance=distance,
                p_values=p_values, random_stats=random_stats)
            data_dict[g_nam] = data
            data_sep_dict[g_nam] = data_sep

        return data_dict, data_sep_dict
            
    def select_by_group(
            self, name, id_group, group_name, group_label, id_label,
            distance=None, p_values=True, random_stats=False):
        """
        Extracts and returns data for one colocalization case (both 
        the individual and all tomos tables) that contain only the data 
        for tomos specified by another table (args id_group, group_name
        group_label and id_label) and distances (arg distance). 

        The colocalization data is obtained from the individual 
        tomos table of the specified colocalization case (arg name).

        This method is the same as (and is based on) select(), except
        that tomo selection is done based on another table (arg id_group)
        that contains group names and tomo ids. In short, tomo ids are 
        found that belong to the specified group (arg group_name).

        More precisely, all rows of the (arg) id_group table that have 
        (arg) group_name value in column (arg) group_label are selected,
        and the tomo ids given in column (arg) id_label of the selected 
        rows are extracted. Method select is then called using the extracted
        tomo ids.
        
        If none of the specified ids exist, (None, None) is returned.

        Arguments:
          - name: name of the colocalization case
          - id_group: (DataFrame) table that contains group names and tomo ids
          - group_name: group name
          - group_label: name of the column of id_group table that contains
          group names
          - id_label: name of the column of id_group table that contains
          tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns: all_tomos_table, individual_tomos_table
        """

        # get tomo ids for the specified group 
        if isinstance(id_group, pd.DataFrame):
            ids = id_group[id_group[group_label]==group_name][id_label]
            ids = ids.unique()
        else:
            raise ValueError("Argumnent id_group has to be pandas.DataFrame") 

        # select data 
        data, data_sep = self.select(
            name=name, ids=ids, distance=distance, p_values=p_values,
            random_stats=random_stats)
            
        return data, data_sep
       
    def select(
            self, name, ids=None, distance=None, p_values=True,
            random_stats=False):
        """
        Extracts and returns data for one colocalization case (both 
        the individual and all tomos tables) that contain only the data 
        for tomos specified by tomo ids (arg ids) and distances (arg 
        distance). 

        The colocalization data is obtained from the individual 
        tomos table of the specified colocalization case (arg name).

        This instance has to contain tables corresponding to arg name. These 
        are returned by self.get_data(name) . 

        The extracted individual tomos table is generated by simply
        selecting the rows of the original individual tomos table.

        Values of  extracted all tomos table are calculated from the 
        extracted individual tomos table. Some columns (like number of 
        subcolumns or number of particles in subcolumns) are obtained
        by simple summation. Values of other columns (like p-values and
        simulation means and stds) are recalculated from the extracted 
        individual tomos table.

        The individual tomos table (pandas.DataFrame specified by arg name) 
        has to be indexed by (tomo) ids or have a column named 'id' that 
        contains ids.

        If none of the specified ids exist, (None, None) is returned.

        Arguments:
          - name: name of the colocalization case
          - ids: list of tomo ids
          - distance: list of distances in nm for which the data 
          is calculated, if None (default) all distances are used
          - p_values: flag indication if p_values should be calculated
          - random_stats: flag indicating whether basic stats are calculated
          for random simulations

        Returns: all_tomos_table, individual_tomos_table
        """

        # get individual tomos data table
        data_orig, data_orig_sep = self.get_data(name)

        # select specified rows
        data_sep = col_func.select_rows(
            data=data_orig_sep, ids=ids, distance=distance)
        if data_sep.shape[0] == 0:
            return None, None

        # get distances
        if distance is None:
            distance = data_sep['distance'].unique()
            distance.sort()

        # combine data from selected tomos 
        _, _, add_columns, array_columns = col_func.set_read_parameters(
            name=name, distances=distance, in_path='_foo', mode=self.mode,
            n_sim=None)
        data_tog = col_func.aggregate(
            data=data_sep, distance=distance, add_columns=add_columns,
            array_columns=array_columns, p_values=p_values,
            p_func=self.p_func, random_stats=random_stats)

        # add columns of original that are not in data_tog, try to
        # preserve order
        orig_only = [
            col for col in data_orig.columns if col not in data_tog.columns]
        orig_only_pos = [data_orig.columns.get_loc(col) for col in orig_only]
        new_cols = data_tog.columns.to_list()
        data = pd.merge(
            left=data_orig[['distance']+orig_only], right=data_tog,
            validate='one_to_one')
        for pos, col in zip(orig_only_pos, orig_only):
            new_cols.insert(pos, col)
        data = data[new_cols]
        
        # return       
        return data, data_sep
                

    ####################################################
    #
    # Colocalization analysis (data processing) of preprocessed
    # coloclization data
    #

    def calculate_ring_density(
            self, names, layer_ind, distances, 
            normalize=False, normalize_index=0):
        """
        Calculates number of particles in a ring divided by ring
        surface area for each ring of one or more colocalizations 
        (arg names). The rings are formed by circles
        with radii obtained from consecutive distances (arg distances).

        Note that the number of particles divided by ring surface area
        has dimensions of but it is not really area concentration
        because the number is divided by surface area of one ring and not
        by the total number of rings. 

        For example, if distances = [5, 10, 15], the density is calculated 
        for the circle of radius 5 nm, and rings 5-10 nm and 10-15 nm.

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before in the module (object) specified by arg module.
        The corresponding variable names have to be of the form 
        colocalization name + '_data'.

        Arg layer_index defines the particle set whose ring surface density 
        is calculated. For example, if name is pre_tether_post, layer_index
        1 selects tether particles and 2 post particles.

        Arguments:
          - names: list of colocalization names
          - layer_index: position within a colocalization name the defines 
          the particle set whose ring surface density is calculated 
          - distances: list of distances
          - module: module where colocalization data tables are defined
          - normalize: if True, the density values are normalized to the
          surface density value of the specified ring (by arg normalize_index)
          - normalize_index: index of the ring that is used for normalization

        Returns: (pd.DataFrame) Table containing the calculated ring surface
        densities where rows are colocalizations (arg names) and columns
        are rings.
        """

        # set module
        #if module is None:
        #    module = sys.modules[__name__]

        # initialize resulting table
        distances_0 = [0] + distances
        ring_labels = [
            '{}-{}'.format(distances_0[i], distances_0[i+1]) 
            for i in range(len(distances))]    
        result = pd.DataFrame({'ring' : ring_labels})
        #result = result.set_index('ring')  bad graph points ordering

        # loop over all colocalizations
        for nam in names:

            # get data and initialize for this colocalization
            data = self.get_data(nam)[0]
            column_name = (
                'n_' + col_func.get_layers(nam)[layer_ind] + '_subcol')
            n_part = data.loc[:, column_name]
            n_part_0 = np.insert(np.array(n_part), 0, 0)
            density = np.zeros(len(distances)) - 1

            # caluclate ring density
            for index in range(len(distances)):
                n_diff = n_part_0[index+1] - n_part_0[index]
                surface = (
                    np.pi * (distances_0[index+1]**2 - distances_0[index]**2))
                density[index] = n_diff / surface
            if normalize:
                density = density / density[normalize_index]

            # put this ring density as one column
            result[nam] = density

        return result

    def plot_ring_density(
            self, names, distances, density=None, layer_ind=None, 
            normalize=False, normalize_index=0,
            ax=None, labels=None, bare=False):
        """
        Plots number of particles in a ring divided by ring
        surface area for each ring of one or more colocalizations 
        (arg names). The rings are formed by circles
        with radii obtained from consecutive distances (arg distances).
        The ring surface densities can be calculated or specified by 
        arg density.

        Note that the number of particles divided by ring surface area
        has dimensions of but it is not really area concentration
        because the number is divided by surface area of one ring and not
        by the total number of rings. 

        For example, if distances = [5, 10, 15], the density is calculated 
        for the circle of radius 5 nm, and rings 5-10 nm and 10-15 nm.

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before in the module (object) specified by arg module.
        The corresponding variable names have to be of the form 
        colocalization name + '_data'.

        If an existing graph is specified by arg ax, the densities are
        plotted on that graph. Otherwise a new graph is made.
        If arg bare is False, graph legend and axis label are added to 
        the graph. 

        Consequently, ax=None and bare=True are used to plot a single 
        graph. For plotting densities of multiple colocalizations
        (that cannot be specified by arg names) the first plot
        should have ax=None and the other the value returned by the
        prvious call, while bare=False should be set only for one call.

        Arg layer_index defines the particle set whose ring surface density 
        is calculated. For example, if name is pre_tether_post, layer_index
        1 selects tether particles and 2 post particles.

        Arguments:
          - names: list of colocalization names
          - layer_index: position within a colocalization name the defines 
          the particle set whose ring surface density is calculated 
          - distances: list of distances
          - density: (pd.DataFrame) ring densities to be plotted, in the 
          same form as the calculated (returned) ring surface densities
          - module: module where colocalization data tables are defined
          - normalize: if True, the density values are normalized to the
          surface density value of the specified ring (by arg normalize_index)
          - normalize_index: index of the ring that is used for normalization
          - ax: an existing graph, or None for a new one
          - bare: Indicates whether graph legend and axis labels are added 
          - labels: (dict) graph labels for colocalizations where keys are 
          colocalization names and values the corresponding labels

        Returns: 
          - ax: this plot
          - table (pd.DataFrame): Table containing the calculated ring 
          surface densities where rows are colocalizations (arg names) 
          and columns are rings.
        """

        # set module
        #if module is None:
        #    module = sys.modules[__name__]

        # initialize 
        if ax is None:
            fig, ax = plt.subplots()
        distances_0 = [0] + distances

        # calculate ring density if not specified
        if density is None:
            density = self.calculate_ring_density(
                names=names, layer_ind=layer_ind, 
                distances=distances, normalize=normalize, 
                normalize_index=normalize_index)

        # plot data on graph
        for nam in names:

            x_ind = range(len(distances))
            if labels is not None:
                lab = labels[nam]
            else:
                lab = nam
            ax.plot(density.index, density[nam], 'x', linestyle='-', label=lab)

        if bare:
            return ax, density

        # add labels and related to the graph
        ax.legend(loc='best')
        ax.set_xlabel('Distance [nm]')
        if normalize:
            ax.set_ylabel('Normalized n particles per ring area')
        else:
            ax.set_ylabel('N particles per ring area [$1/nm^2$]')
        ax.set_xticks(x_ind)
        ax.set_xticklabels(
            ['{}-{}'.format(distances_0[i], distances_0[i+1]) 
             for i in range(len(distances))] )

        return ax, density

    def combine_23(self, name, distance):
        """
        Makes a table that contains colocalization data for one
        3-colocalization and the three corresponding 2-colocalizations, all
        for one distance.

        Colocalization data tables (pandas.DataFrame) have to be 
        defined before as attributes of this object. The corresponding 
        variable names have to be of the form colocalization name + '_data'.

        Arguments:
          - name: 3-colocalization name
          - distance: distance
          - module: module where colocalization data tables are defined

        Returns: (pd.DataFrame) Table where rows correspond to the four 
        colocalizations and the columns are the union of columns of the
        colocalizations.

        """
        pset_names = col_func.get_layers(name=name)
        data_3 = self.get_data(name)[0]
        data_3['name'] = name

        name_01 = pset_names[0] + '_' + pset_names[1]
        data_01 = self.get_data(name_01)[0]
        data_01['name'] = name_01

        name_02 = pset_names[0] + '_' + pset_names[2] 
        data_02 = self.get_data(name_02)[0]
        data_02['name'] = name_02

        name_12 = pset_names[1] + '_' + pset_names[2] 
        data_12 = self.get_data(name_12)[0]
        data_12['name'] = name_12

        data = pd.concat(
            [data_3, data_01, data_02, data_12], ignore_index=True, sort=False)
        data = data[data['distance'] == distance].copy()
        data = data.set_index('name')

        return data

    def calculate_ring_density_23(
            self, name, distances, normalize, normalize_index=0, 
            subtract=False):
        """
        Calculates number of particles in a ring divided by ring
        surface area (ring density) for each ring of the specified 
        3-colocalization (arg name) and for the corresponding 2-coloclazations.

        Note that this ring density has dimensions of but it is not really 
        area concentration because the number is divided by surface area of 
        one ring and not by the total number of rings. 

        The ring density is calculated dor the following:
          - particles of layer 1 for the 3-colocalization
          - particles of layer 2 for the 3-colocalization
          - particles of layer 1 for 2-colocalization between layers 0 and 1
        that are not in the 3-colocalization
          - particles of layer 2 for 2-colocalization between layers 0 and 2
        that are not in the 3-colocalization
          - particles of layer 2 for  2-colocalization between layers 1 and 2

        Layers in a 3-colocalization are labeled from 0 to 2.

        Returns:
           - density: Table containing the calculated densities
         """
        return self.plot_ring_density_23(
            name=name, distances=distances, normalize=normalize, 
            normalize_index=normalize_index, subtract=subtract, plot=False)

    def plot_ring_density_23(
            self, name, distances, normalize, normalize_index=0, 
            subtract=False, plot=True):
        """
        Calculates and plots number of particles in a ring divided by ring
        surface area (ring density) for each ring of the specified 
        3-colocalization (arg name) and for the corresponding 2-coloclazations.

        Note that this ring density has dimensions of but it is not really 
        area concentration because the number is divided by surface area of 
        one ring and not by the total number of rings. 

        The ring density is calculated for the following:
          - particles of layer 1 for the 3-colocalization
          - particles of layer 2 for the 3-colocalization
          - particles of layer 1 for 2-colocalization between layers 0 and 1
        that are not in the 3-colocalization
          - particles of layer 2 for 2-colocalization between layers 0 and 2
        that are not in the 3-colocalization
          - particles of layer 2 for  2-colocalization between layers 1 and 2

        Layers in a 3-colocalization are labeled from 0 to 2.

        Returns:
          - ax: axis
          - density: Table containing the calculated densities
        """

        # particle set and 2-colocalization names
        psets = col_func.get_layers(name)
        names_2c = [
            psets[0] + '_' + psets[1], psets[0] + '_' + psets[2],
            psets[1] + '_' + psets[2]]

        # calculate densities for 3-colocs
        density_1 = self.calculate_ring_density(
            names=[name], layer_ind=1, distances=distances, 
            normalize=normalize, normalize_index=normalize_index)
        density_2 = self.calculate_ring_density(
            names=[name], layer_ind=2, distances=distances, 
            normalize=normalize, normalize_index=normalize_index)

        # calculate subtracted densities for 2-colocs
        density_2c = self.calculate_ring_density(
            names=names_2c, layer_ind=1,
            distances=distances, normalize=normalize, 
            normalize_index=normalize_index)

        # subtract 2-coloc from 3-coloc densities if needed 
        if subtract:
            density_2c[names_2c[0]] = density_2c[names_2c[0]] - density_1[name]
            density_2c[names_2c[1]] = density_2c[names_2c[1]] - density_2[name]
            #density_2c[names_2c[2]] = density_2c[names_2c[2]] - density_2[name]

        # rename 3-coloc columns
        density_1 = density_1.rename(columns={name : psets[1] + ': ' + name})
        density_2 = density_2.rename(columns={name : psets[2] + ': ' + name})

        # rename 2-coloc columns
        layer_indices = [1, 2, 2]
        labels_2c = dict(
            [(nam, '{}: {}'.format(psets[l_ind], nam)) 
                for nam, l_ind in zip(names_2c, layer_indices)])  
        density_2c = density_2c.rename(columns=labels_2c)

        # put all densities into a single table
        densities = pd.concat([density_1, density_2, density_2c], axis=1)
        densities = densities.iloc[:,[0,1,3,5,6,7]].copy()

        if plot: 

            columns = [col for col in densities.columns if col != 'ring']
            labels = dict([(col_nam, col_nam) for col_nam in columns])
            if subtract:
                labels[columns[2]] = columns[2] + ' - 3coloc' 
                labels[columns[3]] = columns[3] + ' - 3coloc' 

            ax, _ = self.plot_ring_density(
                names=columns, density=densities, distances=distances, 
                normalize=normalize, normalize_index=normalize_index,
                labels=labels)

            return ax, densities

        else:
            return densities

    def calculate_fraction_particles_single(self, name, plot=False):
        """
        Calculates fraction of particles present in the specified subcolumn 
        (3-colocalization)
        """

        # set module
        #if module is None:
        #    thismodule = sys.modules[__name__]

        data = self.get_data(name)[0]
        data = data.copy()

        if plot:
            fig, ax = plt.subplots()

        pset_names = col_func.get_layers(name=name)
        for layer_name in set(pset_names):

            subcol_column = 'n_' + layer_name + '_subcol'
            total_column = 'n_' + layer_name + '_total'
            fract_column = 'n_' + layer_name + '_fraction'
            data[fract_column] = data[subcol_column] / data[total_column]

            if plot:
                ax.plot(
                    'distance', fract_column, 'x', data=data, linestyle='-', 
                    label=fract_column)

        if plot:
            return ax, data
        else:
            return data

    def plot_fraction_particles_single(self, name):
        """
        Plots fraction of particles present in specified subcolumn 
        (3-colocalization)
        """
        return self.calculate_fraction_particles_single(
            name=name, plot=True)

    def get_subcol_content(self, names, distance):
        """
        Calculates the number of tether, pre and post complexes contained 
        within specified subcolumns (3-colocalizations) per subcolumn.

        Arguments:
          - names: list of 3-colocalization names
          - distance: distance
          - module: module where the 3-colocalization data are defined  

        Returns: DataFrame containing the means
        """

        result = None
        for nam in names:

            # get column names
            pre_name, tether_name, post_name = col_func.get_layers(nam)
            subcol_name = 'n_subcol'
            tether_sc_name = 'n_' + tether_name + '_subcol'
            pre_sc_name = 'n_' + pre_name + '_subcol' 
            post_sc_name = 'n_' + post_name + '_subcol'

            # get data
            data = self.get_data(nam)[0].copy()
            data = data[data.distance==distance]

            # calculate
            row = pd.DataFrame(
                np.array(
                    data[[pre_sc_name, tether_sc_name, post_sc_name]] 
                    / data[subcol_name].squeeze()),
                columns=['pre_per_sc', 'tether_per_sc', 'post_per_sc'],
                index=[nam])

            # add to resulting table
            try:
                #result = result.append(row, sort=False)
                result = pd.concat([result, row], ignore_index=True)
            except (NameError, AttributeError):
                result = row

        return result

    def get_reaction_states(self, name, distances, subtract=True):
        """
        """

        # get names
        names_all = col_func.get_layers(name=name)
        data_3_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' for name_x in names_all])
        name_01 = names_all[0] + '_' + names_all[1]
        data_01_sc_cols = (
            ['n_subcol'] + 
            ['n_' + name_x + '_subcol' for name_x in names_all[:2]])
        name_02 = names_all[0] + '_' + names_all[2]
        data_02_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' 
                            for name_x in [names_all[0], names_all[2]]])

        data_3 = self.get_reaction_states_single(
            name=name, distances=distances)
        data_01 = self.get_reaction_states_single(
            name=name_01, distances=distances)
        data_02 = self.get_reaction_states_single(
            name=name_02, distances=distances)

        if subtract:
            d_01 = data_01.loc[data_01.index, data_01_sc_cols]
            d_3 = data_3.loc[data_3.index, data_01_sc_cols]
            data_01.loc[data_01.index, data_01_sc_cols] = d_01 - d_3

            d_02 = data_02.loc[data_02.index, data_02_sc_cols]
            d_3 = data_3.loc[data_3.index, data_02_sc_cols]
            data_02.loc[data_02.index, data_02_sc_cols] = d_02 - d_3

        data =  pd.concat(
            [data_3, data_01, data_02], ignore_index=True, sort=False)
        return data

    def get_reaction_states_single(self, name, distances):
        """
        """

        names_all = col_func.get_layers(name=name)

        data_3 = self.get_data(name)[0]
        data_3_sc_cols = (
            ['n_subcol'] + ['n_' + name_x + '_subcol' for name_x in names_all])
        data_3_total_cols = ['n_' + name_x + '_total' for name_x in names_all]
        data_3_cols = data_3_sc_cols + data_3_total_cols

        dists = [0] + distances
        for dist_ind in range(1, len(dists)):

            # copy values for subcolumns
            data_loc = data_3[
                data_3['distance'] == dists[dist_ind]][data_3_cols].copy()
            data_loc['name'] = name
            data_loc['ring'] = '{}-{}'.format(
                dists[dist_ind-1], dists[dist_ind]) 
            data_loc = data_loc[['name', 'ring'] + data_3_cols]
            data_previous_tmp = data_loc.copy()

            # calculate values for rings
            try:
                this = data_loc.loc[data_loc.index[0], data_3_sc_cols]
                prev = data_previous.loc[data_previous.index[0], data_3_sc_cols]
                data_loc.loc[data_loc.index[0], data_3_sc_cols] = this - prev
            except (NameError, AttributeError):
                pass
            data_previous = data_previous_tmp

            # append to data table
            try:
                #data = data.append(data_loc, ignore_index=True)
                data = pd.concat([data, data_loc], ignore_index=True)
            except (NameError, AttributeError):
                data = data_loc.copy()

        return data

    def plot_ring_numbers(
            self, name, distances, layer_index, ring_normalize=False, 
        labels=None, ax=None):
        """
        """

        # get rings data
        rings = self.get_reaction_states(name=name, distances=distances)
        layer_names = col_func.get_layers(name)

        # calculate ring area
        circle_area = np.pi * np.array(distances) ** 2
        inner_circle_area = np.hstack([[0], circle_area[:-1]])
        ring_area = circle_area - inner_circle_area

        if ax is None:
            fig, ax = plt.subplots()

        # 3-colocalization
        col_name = name
        column = 'n_{}_subcol'.format(layer_names[layer_index])
        if labels is None:
            label = '{} 3-col'.format(layer_names[layer_index])
        else:
            label = labels.pop(0)
        rings_one = rings[rings['name']==col_name].copy().reset_index()
        y_data = rings_one[column]
        if ring_normalize:
            y_data = y_data / ring_area
        ax.plot(rings_one.index, y_data, 'x', linestyle='-', label=label)

        # 2-colocalization
        col_name = '{}_{}'.format(layer_names[0], layer_names[layer_index])
        column = 'n_{}_subcol'.format(layer_names[layer_index])
        if labels is None:
            label = '{} 2-col'.format(layer_names[layer_index])
        else:
            label = labels.pop(0)
        rings_one = rings[rings['name']==col_name].copy().reset_index()
        y_data = rings_one[column]
        if ring_normalize:
            y_data = y_data / ring_area
        ax.plot(rings_one.index, y_data, 'x', linestyle='-', label=label)

        # finish plot
        if ring_normalize:
            ax.set_ylabel('Ring normalized N particles [$1/nm^2$]')
        else:
            ax.set_ylabel('Number of particles')
        ax.set_xticks(list(range(rings_one.shape[0])))
        ax.set_xticklabels(rings_one['ring'])
        ax.set_xlabel('Distance [nm]')
        ax.legend(loc='best');

        return ax, rings

    def make_2coloc_significance_table(
            self, names_1, names_2, names_1_col, target_col='distance', 
            target_values=None,  signif_col='p_subcol_combined', 
            signif_value=0.95):
        """
        Considers all 2-colocalization that can be formed between particle 
        sets specified by args names_1 and names_2. For each of them, 
        finds all values of the target column (specified 
        by arg target_col), for which the significance column (specified by 
        arg signif_col) has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return table of distances for which number of 
        2-colocalizations is significant to >95% agains the combined 
        random model:

        self.make_2coloc_significance_table(
            namea_1=pre_names, name_2=tether_names+post_names,
            names_1_col='pre', target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - names_1, names_2: (list) particles sets of layers 1 and 2
          - names_1_col: name of the first (index) column of the table
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: (pandas.DataFrame) table containing selected target values
        """

        # initialize
        res_table = pd.DataFrame()

        # loop over columns defined by names_2
        for nam_2 in names_2:

            # make all values for this column 
            col_data = []
            for nam_1 in names_1:
                #name = nam_1 + '_' + nam_2 + '_data'
                name = self.get_join_name(f'{nam_1}_{nam_2}')
                col_data.append(self.find_significant(
                    name=name, target_col=target_col, 
                    target_values=target_values, signif_col=signif_col, 
                    signif_value=signif_value))

            # add this column to the resulting table
            col_table = pd.DataFrame(
                {names_1_col : names_1, nam_2 : col_data})
            col_table = col_table.set_index(names_1_col)
            res_table = pd.concat([res_table, col_table], axis=1)

        return res_table

    def make_3coloc_significance_table(
            self, names_1, names_2, names_3, names_1_col, 
            target_col='distance', 
            target_values=None,  signif_col='p_subcol_combined', 
            signif_value=0.95):
        """
        Considers all 3-colocalization that can be formed between particle 
        sets specified by args names_1, names_2 and names_3. For each of them, 
        finds all values of the target column (specified 
        by arg target_col), for which the significance column (specified by 
        arg signif_col) has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return table of distances for which number of 
        3-colocalizations is significant to >95% agains the combined 
        random model:

        self.make_3coloc_significance_table(
            namea_1=pre_names, name_2=tether_names, names_3=post_names,
            names_1_col='pre', target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - names_1, names_2, names_3: (list) particles sets of layers 
          1, 2 and 3
          - names_1_col: name of the first (index) column of the table
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: (pandas.DataFrame) table containing selected target values
        """

        # initialize
        res_table = pd.DataFrame()

        # loop over columns defined by names_2
        for nam_2, nam_3 in itertools.product(names_2, names_3):

            # make all values for this column 
            col_data = []
            for nam_1 in names_1:
                #name = nam_1 + '_' + nam_2 + '_' + nam_3 + '_data'
                name = self.get_join_name(f'{nam_1}_{nam_2}_{nam_3}')
                col_data.append(self.find_significant(
                    name=name, target_col=target_col, 
                    target_values=target_values, signif_col=signif_col, 
                    signif_value=signif_value))

            # add this column to the resulting table
            col_table = pd.DataFrame(
                {names_1_col : names_1, nam_2+'_'+nam_3 : col_data})
            col_table = col_table.set_index(names_1_col)
            res_table = pd.concat([res_table, col_table], axis=1)

        return res_table

    def find_significant(
            self, name, target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95):
        """
        Given a colocalization table specified by arg name, finds and 
        returns all values of the target column (specified by arg target_col),
        for which the significance column (specified by arg signif_col) 
        has value greater than arg signif_value. 

        If arg target_values is not None, only the colocalizations (table
        rows) that have one of the specified target_values in target_col 
        column are considered.

        Common usage: To return distances for which number of colocalizations
        is significant to >95% agains the combiner random model:

        self.find_significant(
            name=coloc_name, target_col='distance', target_values=None, 
            signif_col='p_subcol_combined', signif_value=0.95)

        Arguments:
          - name: colocalization name
          - target_col: name of the target column
          - target_values: list or array of target values, or None for 
          all values
          - signif_col: name of the significance column
          - signif_value: threshold significance value 

        Returns: ndarray of selected target values
        """

        # find all distances where significant
        data = getattr(self, name)
        sig_data = data[data[signif_col] > signif_value]
        result = np.array(sig_data[target_col])

        # restrict to specified distances
        if target_values is not None:
            result = np.array(
                [one_res for one_res in result if res_dist in target_values])

        return result

    def make_3coloc_table(
            self, names_1, names_2, names_3, select_value,
            select_col='distance', target_col='n_subcol',
            fraction=False, totals=False):
        """
        Considers all 3-colocalization that can be formed between particle 
        sets specified by args names_1, names_2 and names_3. Makes a table 
        that contains one value from each of the 3-colocalization.

        The selected value is found at the intersection of the column 
        specified by arg target_col and the row specified by args 
        select_col and select_value.

        Note: The select_value is expected to be unique in the select_col.

        Note: In the current implementation, arg names_2 should contain 
        only one element. 

        Arguments:
          - names_1, names_2, names_3: (list) particles sets of layers 
          1, 2 and 3
          - target_col: name of the target column
          - select_column: name of the column used to select one row
          - select_value: value in the select_column that selects one row
          - target_values: list or array of target values, or None for 
          - fraction: if True, the selected values are divided by the
          total number of the corresponding particles
          - totals: if True, totals (marginal values) are added for 
          all rows and columns

        Returns (pandas.DataFrame) table containing the selected values
        """

        # loop over columns
        table = pd.DataFrame()
        for post in names_3:

            # loop over rows
            column_values = []
            for pre in names_1:

                # get value for the selected row of this column
                name = pre + '_' + names_2[0] + '_' + post
                name = self.get_join_name(name)
                coloc_local = getattr(self, name)
                row = (coloc_local[select_col] == select_value)
                value = np.array(coloc_local.loc[row, target_col])[0]

                # normalize to total number, if needed
                if fraction:
                    pre_total_column = 'n_' + pre + '_total'
                    post_total_column = 'n_' + post + '_total'
                    pre_total = np.array(
                        coloc_local.loc[row, pre_total_column])[0]
                    post_total = np.array(
                        coloc_local.loc[row, post_total_column])[0]
                    value = value / float(pre_total * post_total)
                column_values.append(value)

            # add this column to resulting table
            column_tab = pd.DataFrame({'pre' : names_1, post : column_values})
            column_tab = column_tab.set_index('pre')
            table = pd.concat([table, column_tab], axis=1)

        # add totals if needed
        if totals:
            table.loc[:, 'total'] = table.sum(axis=1)
            table.loc['total', :] = table.sum(axis=0)

        return table

    def make_table_names(self, layers):
        """
        Makes standard table names from layer names.

        Specifically, the name of the table where the data of all tomograms 
        is combined is:

          layers[0]_layers[1]..._join_suffix

        and the name of the table containing the data of all individual 
        tomograms is:

          layers[0]_layers[1]..._individual_suffix

        Arguments:
          - layers: (list) layer_names

        Needs:
          - self.join_suffix: suffix for tables containing combined data
          - self.individual_suffix: suffix for individual tomograms table

        Returns [combined_tomo_data_table, individual_tomo_data_table]
        """
        common = ''
        for lay in layers:
            common += lay + '_'
        result = (common + self.join_suffix, common + self.individual_suffix)
        return result

    def combine_permuted(
            self, names, p_columns=['p_subcol_normal', 'p_subcol_other'],
            out_column_suffix='_3', check_column='distance', 
            update=True, ignore=True, verbose=True):
        """
        Combines averages of p_values from all of the collocalization data 
        tables specifed by arg name and from other existing tables 
        obtained by permuting the order of particle sets comprising 
        specified table names.

        Based on combine_permuted_one().

        For each table (element of arg names), all different combinations 
        of the particle sets contained in the name of this table.

        Implemented for combined tomos tables only.

        Implemented for 3-colocalizations only.

        Common usage:

          self.combine_permuted(
            names=['set1a_set2a_set3a', 'set1b_set2b_set3b'],  
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3')

        In this case, it first finds the existing tables that start with
        'set2a' and 'set3a', such as self.set2a_set1a_set3a_data and
        self.set3a_set1a_set2a_data. Then it makes the following
        averages:
          - 'p_subcol_normal_3' from column 'p_subcol_normal' of
          the three tables
          - 'p_subcol_other_3' from column 'p_subcol_other' of
          the three tables
          - 'p_subcol_combined_3: from the above two
        This procedure is repeated all elements of arg names.

        Returns a list of tables corresponding to arg names. Each table
        contains all the data of the corresponding input table,
        where the calculated averages are added as new columns. If arg
        update is True, the input tables are updated to include the 
        calculated columns. Otherwise new columns are returned
        and the original ones are left unchanged. 

        Arguments:
          - names: (list) names of the firstinput tables
          - p_columns: list of column names that contain p_values to be
          combined
          - p_column_combined: name of the column that normaly contains
          p_values from the normal and other random models
          - out_column_suffix: names of new columns are obtained by 
          adding this argument to the original column name
          - check_column: name of the column that should be the same 
          in all tables
          - update: flag indicating whether the input table is updated,
          or a new table is returned
          - verbose: flag indicating if a message is printed when a 
          table is not found

         Returns: list of tables corresponding to atg names, where each 
        element contains all data of the corresponding input table and the
        new columns, only if update is False.
        """

        res = []
        for nam in names:
            try:
                res_one = self.combine_permuted_one(
                    name=nam, other=None, p_columns=p_columns,
                    out_column_suffix=out_column_suffix, 
                    check_column=check_column, update=update, verbose=verbose)
            except ValueError:
                if not ignore: raise
            if not update:
                res.append(res_one)

        return res

    def combine_permuted_one(
            self, name, other=None, 
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3', 
            check_column='distance', update=True, verbose=True):
        """
        Combines averages of p_values from collocalization data table 
        specifed by arg name and from other existing tables obtained 
        by permuting the order of particle sets comprising arg name.

        If arg other is None, all different combinations of the particle
        sets contained in arg name are checked. Otherwise, only the 
        tables specified by arg order are used. These tables have to 
        contain collocalization data for the same particle sets but in 
        different order.

        Implemented for combined tomos tables only.

        Implemented for 3-colocalizations only.

        Common usage:

          self.combine_permuted_one(
            name='set1_set2_set3', other=None, 
            p_columns=['p_subcol_normal', 'p_subcol_other'],
            p_column_combined='p_subcol_combined', out_column_suffix='_3')

        In this case, it first finds the existing tables that start with
        'set2' and 'set3', such as self.set2_set1_set3_data and
        self.set3_set1_set2_data. Then it makes the following
        averages:
          - 'p_subcol_normal_3' from column 'p_subcol_normal' of
          the three tables
          - 'p_subcol_other_3' from column 'p_subcol_other' of
          the three tables
          - 'p_subcol_combined_3: from the above two

        Returns a table that contains all the data of the (arg) name table,
        where the calculated averages are added as new columns. If arg
        update is True, the initial table specified by atg name is
        updated to the returned column. Otherwise a new column is returned
        and the original one is left unchanged. 

        Arguments:
          - name: name of the input table
          - other: list conatining names of other tables, or None to find
          the other tables based on the arg name 
          - p_columns: list of column names that contain p_values to be
          combined
          - p_column_combined: name of the column that normaly contains
          p_values from the normal and other random models
          - out_column_suffix: names of new columns are obtained by 
          adding this argument to the original column name
          - check_column: name of the column that should be the same 
          in all tables
          - update: flag indicating whether the input table is updated,
          or a new table is returned
          - verbose: flag indicating if a message is printed when a 
          table is not found

        Returns: table that contains all data of name and the new columns.
        """

        # get and check layers
        layers = col_func.get_layers(name=name)
        if len(layers) != 3:
            err_msg = (
                'P-values could not be combined because colocalization ' 
                + name + ' does not have exactly three layers')
            if verbose:
                print(err_msg)
            raise ValueError(err_msg)
        lay_1, lay_2, lay_3 = layers

        # find colocalizations to be combined
        tables = [self.get_data(name)[0]]
        if not update:
            tables[0] = tables[0].copy()
        if other is None:

            # try all possible combinations of layers
            try_name = self.make_table_names(layers=[lay_2, lay_1, lay_3])[0]
            if hasattr(self, try_name):
                tables.append(getattr(self, try_name))
            else:
                try_name = self.make_table_names(
                    layers=[lay_2, lay_3, lay_1])[0]
                if hasattr(self, try_name):
                    tables.append(getattr(self, try_name))
            try_name = self.make_table_names(layers=[lay_3, lay_1, lay_2])[0]
            if hasattr(self, try_name):
                tables.append(getattr(self, try_name))
            else:
                try_name = self.make_table_names(
                    layers=[lay_3, lay_2, lay_1])[0]
                if hasattr(self, try_name):
                    tables.append(getattr(self, try_name))

        else:
            tables = tables + [self.get_data(nam)[0] for nam in other]

        # check tables
        for ind in range(1,len(tables)):
            if not (tables[0][check_column] == tables[ind][check_column]).all():
                raise ValueError("Tables don't have the same row order")

        # combine p_values
        new_p_combined_col = p_column_combined + out_column_suffix
        for p_col in p_columns:
            new_p_col = p_col+out_column_suffix
            tables[0][new_p_col] = tables[0][p_col]
            for ind in range(1,len(tables)):
                tables[0][new_p_col] += tables[ind][p_col]
            tables[0][new_p_col] = tables[0][new_p_col] / float(len(tables))
            try:
                tables[0][new_p_combined_col] += tables[0][new_p_col]
            except KeyError:
                tables[0][new_p_combined_col] = tables[0][new_p_col]
        tables[0][new_p_combined_col] = (
            tables[0][new_p_combined_col] / float(len(p_columns)))

        return tables[0]

    def test_2_in_3_independence(
            self, names, join='inner', select_column='distance', value=None, 
            coloc_column='coloc'):
        """
        For all 3-colocalizations specified in arg names, tests whether 
        the corresponding 2-colocalizations that constitute these 
        3-colocalization are statistically independent.

        Uses test_2_in_3_independence_single()

        Specifically, for 3-colocalization set1_set2_set3, it calculates
        probabilities that a set1 particle is in 2-colocalizations
        set0_set1 and set0_set2. Based on these, the expected numbers of 
        set1 particles that belong and do not belong to set1_set2_set3 
        are calculated. The expecttion assumes that the 2- and 
        3-colocalizations are independent. The expected values are compared
        using chi-square with the real velues of set1_set2_set3.

        If args select_column and value are not None, returns only those
        rows where the selected column has the given value.

        Argument:
          - name: 3-colocalizstion name
          - join: the same as in pandas.concatenate(), controls how
          the tables corresponding to the individual elements of arg names
          are concatenated
          - select_column: name of the selection column
          - value: selection value
          - coloc_column: name of the column in the returned table that
          shows the individual 3-colocalization names

        Returns table containing expected, obtained and chi-square values
        """
        
        # test all colocalizations
        for nam in names:
            one = self.test_2_in_3_independence_single(name=nam)
            one[coloc_column] = nam
            try:
                result = pd.concat([result, one], join=join)
            except NameError:
                result = one

        # reorder columns
        old_cols = result.columns
        new_cols = [old_cols[0]] + [old_cols[-1]] + list(old_cols[1:-1])
        result = result[new_cols]

        # select rows
        if (select_column is not None) and (value is not None):
            result = result[result[select_column]==value]

        return result

    def test_2_in_3_independence_single(self, name):
        """
        Tests whether the 2-colocalizations that constitute the 
        3-colocalization specified by arg name are independent.

        Specifically, for 3-colocalization set1_set2_set3, it calculates
        probabilities that a set1 particle is in 2-colocalizations
        set0_set1 and set0_set2. Based on these, the expected numbers of 
        set1 particles that belong and do not belong to set1_set2_set3 
        are calculated. The expecttion assumes that the 2- and 
        3-colocalizations are independent. The expected values are compared
        using chi-square with the real velues of set1_set2_set3.

        Argument:
          - name: 3-colocalizstion name

        Returns table containing expected, obtained and chi-square values
        """

        # get particle names and data
        layers = col_func.get_layers(name)
        data = self.get_data(name)[0]
        data_01 = self.get_data(layers[0] + '_' + layers[1])[0]
        data_02 = self.get_data(layers[0] + '_' + layers[2])[0]
        n_lay0_total = 'n_'+layers[0]+'_total'

        # 3-coloc results
        res = data.loc[:,['distance', 'n_subcol']].copy()
        res['n_subcol F'] = (
            data.loc[:, 'n_'+layers[0]+'_total'] - data.loc[:, 'n_subcol'])

        # calculate expected 3-coloc results based on 2-colocs 
        res[layers[1]+' fract'] = (
            data_01.loc[:,'n_subcol'] / data_01.loc[:, n_lay0_total])
        res[layers[2]+' fract'] = (
            data_02.loc[:,'n_subcol'] / data_02.loc[:, n_lay0_total])
        res['expected T'] = (
            res[layers[1]+' fract'] * res[layers[2]+' fract'] 
            * data[n_lay0_total])
        res['expected F'] = data[n_lay0_total] - res['expected T']

        # stats
        for row in res.iterrows():
            chisq, pval = pyto.util.scipy_plus.chisquare_2(
                [row[1]['n_subcol'], row[1]['n_subcol F']],
                [row[1]['expected T'], row[1]['expected F']])
            res.loc[row[0], 'chisq'] = chisq
            res.loc[row[0], 'p-value'] = pval
        
        return res

    def enrichment_single(self, name, ref_name, subcol, layer_index):
        """
        Calculates whether particle type (set) in a 3-colocalization 
        (arg name) is enriched in respect to a reference particle set 
        in a reference 3-colocalization (arg ref_name). 

        For example, it can calculate wheteher a in 3-colocalization 
        preb-tether-a is enriched in respect to pstb in reference 
        3-colocalizations preb-tether-pstb.

        Uses chi-square test to compare the observed number of particles 
        that belong and not belong to a 3-colocalization with the expected 
        values. The expected values are calculated from the observed number 
        of refrerence particles in the reference 3-colocalization and the 
        total number of particles in the set that is analyzed and in the 
        reference set. 

        If arg subcol is True, observed and expected number of 
        3-colocalizations are statistically compared. For the total number 
        of analyzed and reference particles, layer 0 is used:

        Specifically:

          1) If subcol is False, name is preb_tether_n, ref_name is 
          preb_tether_pstb and layer_index is 2 the following quantities 
          are used:
              - number of particles n in preb_tether_n (observed value)
              - number of particles pstb in preb_tether_pstb
              - total number of n
              - total number of pstb

          2) If subcol is False, name is preb_tether_n, ref_name is 
          pre_tether_pst and layer_index is 1 the following quantities 
          are used:
              - number of particles preb in preb_tether_n (observed value)
              - number of particles pre in pre_tether_pst
              - total number of n
              - total number of pstb

          3) If subcol is True, name is preb_tether_a, ref_name is 
          pre_tether_pst:
              - number of 3-colocalizations in preb_tether_n (observed value)
              - number of 3-colocalization in preb_tether_pstb
              - total number of preb
              - total number of pre    

        Colocalization data tables (pandas.DataFrame) have to be defined before.
        The corresponding variable names have to be of the form name + '_data'.

        Arguments:
          - name: name of the analyzed 3-colocalization
          - ref_name: name of the reference 3-colocalization
          - subcol: flag indicating whether the number of 3-colocalizations
          is compared, or the number of particle in 3-colocalizations
          - layer_index: index that defines particle layer

        Returns table (pandas.DataFrame) that contains statistical observed,
        expected and statistical comparison values.
        """

        # get data
        data = self.get_data(name)[0]
        ref_data = self.get_data(ref_name)[0]

        # get names of columns that are red
        if subcol:

            layer_index = 0

            name_layer = col_func.get_layers(name)[layer_index]
            name_col = 'n_subcol'
            name_total_col = 'n_' + name_layer + '_total'

            ref_layer = col_func.get_layers(ref_name)[layer_index]
            ref_col = 'n_subcol'
            ref_total_col = 'n_' + ref_layer + '_total'

        else:

            name_layer = col_func.get_layers(name)[layer_index]
            name_col = 'n_' + name_layer + '_subcol'
            name_total_col = 'n_' + name_layer + '_total'

            ref_layer = col_func.get_layers(ref_name)[layer_index]
            ref_col = 'n_' + ref_layer + '_subcol'
            ref_total_col = 'n_' + ref_layer + '_total'

        # get names of columns to be made in the result table
        real_t_col = 'real_t'
        real_f_col = 'real_f'
        expected_t_col = 'exp_t'
        expected_f_col = 'exp_f'

        # put real and expected values in the result table
        result = data[['distance', 'n_subcol']].copy()
        result[real_t_col] = data[name_col]
        result[real_f_col] = data[name_total_col] - result[real_t_col]
        result[expected_t_col] = (
            ref_data[ref_col] * data[name_total_col] / ref_data[ref_total_col])
        result[expected_f_col] = data[name_total_col] - result[expected_t_col]

        # do stats
        for ind, row in result.iterrows():
            f1 = [np.array(row[real_t_col]), np.array(row[real_f_col])]
            f2 = [np.array(row[expected_t_col]), np.array(row[expected_f_col])]
            chi_val, p_value = pyto.util.scipy_plus.chisquare_2(f1, f2)
            result.loc[ind, 'chi2'] = chi_val
            result.loc[ind, 'p_value_chi2'] = p_value

        return result       
